{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T10:45:20.114857Z",
     "start_time": "2025-07-05T10:45:20.112339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# --- DEBUGGING STEP ---\n",
    "# Print the current working directory to see where Python is looking.\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variables\n",
    "# The string \"GENAI_API_KEY\" must match the variable name in your .env file\n",
    "api_key = os.getenv(\"GENAI_API_KEY\")\n",
    "\n",
    "# Check if the API key is loaded correctly\n",
    "if not api_key:\n",
    "    raise ValueError(\"No API key found. Please set the GENAI_API_KEY in your .env file.\")\n"
   ],
   "id": "8af53abd4cf404f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/lewis/github/rag-strategies\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-05T10:04:23.910402Z",
     "start_time": "2025-07-05T10:04:09.520353Z"
    }
   },
   "source": [
    "from google import genai\n",
    "from google.genai import types\n",
    "import pathlib\n",
    "import httpx\n",
    "\n",
    "client = genai.Client(api_key=api_key)\n",
    "\n",
    "# Retrieve and encode the PDF byte\n",
    "file = [\"firds_reference_data_functional_specifications_v2.10.pdf\"]\n",
    "current_dir = os.getcwd()\n",
    "doc_path = os.path.join(current_dir, \"resources\", file[0])\n",
    "filepath = pathlib.Path(doc_path)\n",
    "\n",
    "prompt = \"Show me the entire table of Annex 1c: Reference Data Content and Consistency Validation Rules\"\n",
    "response = client.models.generate_content(\n",
    "  model=\"gemini-2.5-flash\",\n",
    "  contents=[\n",
    "      types.Part.from_bytes(\n",
    "        data=filepath.read_bytes(),\n",
    "        mime_type='application/pdf',\n",
    "      ),\n",
    "      prompt])\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the entire table of **Annex 1c: Reference Data Content and Consistency Validation Rules** from page 185 of the document:\n",
      "\n",
      "**TABLE 33 - REFERENCE DATA CONTENT AND CONSISTENCY VALIDATION RULES**\n",
      "\n",
      "| Control executed by the system | Error code | Error Message | Concerned Fields |\n",
      "| :----------------------------- | :--------- | :------------ | :--------------- |\n",
      "| The value of “Instrument Classification” shall be a valid ISO 10962 code and shall be covered by at least one of the CFI constructs in the CFI-based validation matrix. | INS-101 | The CFI code is not valid against the CFI based validation matrix. | RTS field 3 against the list of valid CFI codes table and against the list of CFI Construct (Primary Key) in the CFI based validation table |\n",
      "| Check that Mandatory fields are reported according to “CFI-based validations table”. | INS-102 | The following mandatory fields are not reported: “List of RTS23 number Id of missing field(s)”. | RTS field 3 vs all other RTS fields |\n",
      "| Check that Non-Applicable fields (N/A) are not reported according to “CFI-based validations table”. | INS-103 | The following Non-Applicable fields are wrongly reported: “List of RTS23 number Id of N/A field(s)”. | All RTS fields |\n",
      "| **The following checks are performed only in case checks above are passed.** | | | |\n",
      "| Check that that a record (ISIN, MIC) is not reported twice in the same file. | INS-104 | The following records are reported twice in the same file. | RTS field 1,6 |\n",
      "| The MIC identifier in the TradingVenueRelatedAttributes block shall exist in the Trading venue mapping view which satisfies the following conditions: ValidityStartDate is prior or equal to the current date and (ValidityEndDate is NULL OR is later or equal to the current date). | INS-105 | The Trading Venue field contains an invalid MIC code. | RTS field 6 |\n",
      "| The Reporting entity identification associated to the MIC [field 6] in Reporting Flow view (TV / SI MIC) is equal to the Reporting Entity identifier in the header of the XML file. | INS-107 | “Trading Venue” field is not registered at ESMA or is not reported by the right reporting entity. | Reporting Entity <br> RTS field 6 |\n",
      "| The Strike Price Currency Code shall exist as an active ISO 4217 Currency Code in the currency reference data table (based on records with ValidityEndDate is NULL. | INS-108 | The Strike Price Currency Code is incorrect. | RTS field 32 |\n",
      "| The Notional Currency 1 Code shall exist as an ISO 4217 Currency Code in the currency reference table (based on records which ValidityEndDate is NULL or PreEuroFlag is TRUE). | INS-109 | The Notional Currency 1 Code is incorrect. | RTS field 13 |\n",
      "| The Notional Currency 2 Code shall exist as an ISO 4217 Currency Code in the currency reference table (based on records which ValidityEndDate is NULL or PreEuroFlag is TRUE). | INS-110 | The Notional Currency 2 Code is incorrect. | RTS field 42, 47 |\n",
      "| The Currency of nominal value shall exist as an ISO 4217 Currency Code in the currency reference table (based on records which ValidityEndDate is NULL or PreEuroFlag is TRUE). | INS-111 | The Currency of nominal value is incorrect. | RTS field 16 |\n",
      "| The value of the “Issuer Identifier” shall exist in the LEI reference table and comply with the following conditions: (ValidityEndDate is NULL OR date of termination of the respective record is between any period specified by ValidityStartDate and ValidityEnddate in LEI reference table for this LEI ) AND register status in {“Issued\", \"Lapsed\", \"Pending transfer\", \"Pending archival}. | INS-112 | The LEI provided for “Issuer Identifier” is invalid. | RTS field 5 |\n",
      "| The value of the “Direct Underlying issuer” shall exist in the LEI reference table and comply with the following conditions: (ValidityEndDate is NULL OR date of termination of the respective record is between any period specified by ValidityStartDate and ValidityEnddate in LEI reference table for this LEI ) AND register status in {“Issued\", \"Lapsed\", \"Pending transfer\", \"Pending archival}. | INS-113 | The LEI provided for “Direct Underlying Issuer” is invalid. | RTS field 27a, 27b |\n",
      "| Check the last digit of the ISIN code of the “instrument identification code” according to the algorithm of ISIN validation. | INS-114 | The ISIN code of the instrument identification code is invalid. | RTS field 1 |\n",
      "| Check the last digit of the ISIN code of the “underlying instrument” should be valid according to the algorithm of ISIN validation. | INS-115 | The ISIN code of the underlying is invalid. | RTS field 26a, 26b, 26c |\n",
      "| Check the last digit of the ISIN code of the Identifier of the “Index/Benchmark of a floating rate Bond” should be valid according to the algorithm of ISIN validation. | INS-116 | The ISIN code of the Index/Benchmark of a floating rate Bond is invalid. | RTS field 19 |\n",
      "| The “Date of admission to trading or date of First trade” should a valid date and in a sensible range (no prior than 31-12-1899). | INS-117 | The “Date of admission to trading or date of First trade” is not a consistent date. | RTS field 11 |\n",
      "| The Termination Date should a valid date and in a sensible range (no prior than 31-12-1899). | INS-118 | The Termination Date is not a consistent date. | RTS field 12 |\n",
      "| The Termination Date should be equal to or later than the “Date of admission to trading or date of First trade”. | INS-119 | The Termination Date is earlier than the “Date of admission to trading or date of First trade”. | RTS field 11, 12 |\n",
      "| The Maturity Date should a valid date and in a sensible range (no prior than 31-12-1899). | INS-120 | The Maturity Date is not a consistent date. | RTS field 15 |\n",
      "| The Maturity Date should be equal to or later than “Date of admission to trading or date of First trade”. | INS-121 | The Maturity Date and Date of admission to trading or date of First trade are not consistent. | RTS field 11, 15 |\n",
      "| The Expiry Date should a valid date and in a sensible range (no prior than 31-12-1899). | INS-122 | The Expiry Date is not a consistent date. | RTS field 24 |\n",
      "| The Expiry date should be equal to or later than the “Date of admission to trading or date of First trade”. | INS-123 | The Expiry Date and The Date of admission to trading or date of First trade are not consistent. | RTS field 11, 24 |\n",
      "| Field “Option Type” shall only contain value “PUTO” when the “Instrument Classification” refers to the following CFI Codes: OP\\*\\*\\*\\* (Put Options). | INS-124 | Invalid “PUTO” Option Type | RTS field 3, 30 |\n",
      "| Field “Option Type” shall only contain value “CALL” when the “Instrument Classification” refers to the following CFI Codes: OC\\*\\*\\*\\*(Call Options). | INS-125 | Invalid “CALL” Option Type | RTS field 3, 30 |\n",
      "| The termination date should be populated in case Maturity date/Expiry date is populated and is strictly earlier than the current reporting date. | INS-126 | The Termination date is not populated for an expired/matured instrument. N.B.: that check if failed generates a warning only. | RTS field 12, {15 or 24} |\n",
      "| The termination date should be earlier or equal in case Expiry date/Maturity date is populated. | INS-127 | The Termination date and Expiry date/Maturity date are not consistent. N.B.: that check if failed generates a warning only. | RTS field 12, {15 or 24} |\n",
      "| The field listed in Table 1 BRD 43. shall be consistent with the values provided by the Relevant competent Authority. | INS-128 | The following fields are not consistent with the one provided by RCA :<<Upcoming RCA>>, RCA\\_MIC :<<MIC>>(<<MIC’s country>>): List of RTS23 number Id of missing field(s)”. N.B.: that check if failed generates a warning only. | RTS fields used for consistency checks as stated in Table 21 - RTS23 Fields table. |\n",
      "| The currency of the Total issued nominal amount shall be the same as the currency of nominal value | INS-129 | The currency of the Total issued nominal amount is not the same as the currency of nominal value | RTS Field 14. Currency <br> RTS Field 16. |\n",
      "| The ISIN-MIC combination, received for a cancellation record, should exists in FIRDS DB. | INS-130 | The ISIN-MIC combination, received from a cancellation record, doesn’t exists in FIRDS DB | RTS field 1,6 |\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T11:14:38.283294Z",
     "start_time": "2025-07-05T11:14:38.280938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from fpdf import FPDF\n",
    "\n",
    "# LangChain components\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "\n",
    "# Define constants\n",
    "file = [\"firds_reference_data_functional_specifications_v2.10.pdf\"]\n",
    "current_dir = os.getcwd()\n",
    "PDF_PATH = os.path.join(current_dir, \"resources\", file[0])\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "USER_QUERY = \"Show me the entire table of Annex 1c: Reference Data Content and Consistency Validation Rules\"\n",
    "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\""
   ],
   "id": "d415a3e8622d61e3",
   "outputs": [],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T11:25:51.545095Z",
     "start_time": "2025-07-05T11:25:39.670521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "#  STEP 1: LOAD AND CHUNK THE DOCUMENT\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Step 1: Loading and Chunking PDF ---\")\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "documents = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=800,\n",
    "    chunk_overlap=100,\n",
    "    length_function=len,\n",
    ")\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "print(f\"PDF loaded and split into {len(chunks)} chunks.\")\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 2: EMBED THE CHUNKS\n",
    "# ==============================================================================\n",
    "print(f\"\\n--- Step 2: Embedding Chunks using '{EMBEDDING_MODEL_NAME}' ---\")\n",
    "# This will download the model from Hugging Face on its first run.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "print(\"Embedding model loaded.\")\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 3: STORE IN A VECTOR DATABASE\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Step 3: Storing chunks in FAISS in-memory vector database ---\")\n",
    "# The from_documents method handles embedding and storing in one step.\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "print(\"Chunks embedded and stored in FAISS.\")\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 4: RETRIEVE RELEVANT CHUNKS\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Step 4: Retrieving Top 10 Chunks via Similarity Search ---\")\n",
    "print(f\"\\nUser Query: \\\"{USER_QUERY}\\\"\")\n",
    "\n",
    "# Retrieve the top 5 most similar chunks\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "print(\"\\n--- Top 10 Retrieved Chunks ---\")\n",
    "initial_results = base_retriever.get_relevant_documents(USER_QUERY)\n",
    "print(f\"\\n--- Top 5 Initial Results (from vector search alone) ---\")\n",
    "for i, chunk in enumerate(initial_results[:5], 1):\n",
    "    print(f\"\\n--- Initial Result {i} ---\\n\")\n",
    "    print(chunk.page_content)"
   ],
   "id": "2c4c82aab6b0488e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Loading and Chunking PDF ---\n",
      "PDF loaded and split into 625 chunks.\n",
      "\n",
      "--- Step 2: Embedding Chunks using 'sentence-transformers/all-mpnet-base-v2' ---\n",
      "Embedding model loaded.\n",
      "\n",
      "--- Step 3: Storing chunks in FAISS in-memory vector database ---\n",
      "Chunks embedded and stored in FAISS.\n",
      "\n",
      "--- Step 4: Retrieving Top 10 Chunks via Similarity Search ---\n",
      "\n",
      "User Query: \"Show me the entire table of Annex 1c: Reference Data Content and Consistency Validation Rules\"\n",
      "\n",
      "--- Top 10 Retrieved Chunks ---\n",
      "\n",
      "--- Top 5 Initial Results (from vector search alone) ---\n",
      "\n",
      "--- Initial Result 1 ---\n",
      "\n",
      "ESMA REGULAR USE \n",
      " \n",
      " \n",
      "33 / 216 \n",
      "Upcoming RCA The country of the Relevant Competent Authority of that instrument, as last \n",
      "determined by the system for the upcoming publication. \n",
      "Free-text fields \n",
      "used for \n",
      "consistency \n",
      "checks \n",
      " “Free-text fields used for consistency checks” fields in “RTS23 Fields table” \n",
      "as listed in section 6.9 RTS23 Fields table. \n",
      "Non-free-text \n",
      "fields used for \n",
      "consistency \n",
      "checks \n",
      " “Non-free-text fields used for consistency checks ” fields in “RTS23 Fields \n",
      "table” as listed in section 6.9 RTS23 Fields table.  \n",
      "TABLE 6 - FIELDS OF REFERENCE FIELDS TABL E \n",
      " \n",
      "Finally, as per section 3.3.10, the ESMA system updates, recursively based on the already existing \n",
      "records, a new table called “Consistent Reference Data Table ” 4  as follows: for each record\n",
      "\n",
      "--- Initial Result 2 ---\n",
      "\n",
      "ESMA REGULAR USE \n",
      " \n",
      " \n",
      "198 / 216 \n",
      "15 Annex 5 ISO reference data tables \n",
      "15.1 Country reference data table \n",
      "Field Name M/O Data field \n",
      "description \n",
      "Data field \n",
      "Values \n",
      "ISO Description Source \n",
      "CountryCode M 2(a)  ISO \n",
      "3166 \n",
      "The 2-character ISO Country Code \n",
      "identifier. \n",
      "• data provider \n",
      "• ESMA manual update \n",
      "CountryName M 70(z)   The ISO description of the country \n",
      "name. \n",
      "• data provider \n",
      "• ESMA manual update \n",
      "EEACountryFlag M TRUEFALSE \n",
      "Indicator TRUE/FALSE  Flag which indicates whether the \n",
      "Country is EEA. \n",
      "• ESMA manual update  \n",
      "• Default value is FALSE \n",
      "ValidityStartDate M \n",
      "Date \n",
      "YYYYMMDD \n",
      "  Date at which the record becomes \n",
      "valid  Generated by the ESMA System \n",
      "ValidityEndDate O \n",
      "Date \n",
      "YYYYMMDD \n",
      "  Date of which the records ends to be \n",
      "valid Generated by the ESMA System \n",
      "LastUpdatedDate M\n",
      "\n",
      "--- Initial Result 3 ---\n",
      "\n",
      "ESMA REGULAR USE \n",
      " \n",
      " \n",
      "183 / 216 \n",
      " \n",
      "9 Annex 1c: Reference Data Content and \n",
      "Consistency Validation Rules  \n",
      "Control executed by the system \n",
      "Error \n",
      "code \n",
      "Error Message \n",
      "Concerned \n",
      "Fields \n",
      "The value of “Instrument Classification” shall \n",
      "be a valid ISO 10962 code and shall be \n",
      "covered by at least one of the CFI constructs \n",
      "in the CFI-based validation matrix. \n",
      "INS-101 \n",
      "The CFI code is not valid \n",
      "against the CFI based \n",
      "validation matrix. \n",
      "RTS field 3 against the list of \n",
      "valid CFI codes table and \n",
      "against the list of CFI Construct \n",
      "(Primary Key) in the CFI based \n",
      "validation table \n",
      "Check that Mandatory fields are reported \n",
      "according to “CFI-based validations table”. \n",
      "INS-102 The following mandatory \n",
      "fields are not reported: \n",
      "“List of RTS23 number Id \n",
      "of missing field(s)”.\n",
      "\n",
      "--- Initial Result 4 ---\n",
      "\n",
      "ESMA REGULAR USE \n",
      " \n",
      " \n",
      "35 / 216 \n",
      "In addition, the system shall have mechanisms in place to avoid that interfacing systems needing \n",
      "access reference data during the 00:00 – 08:00 period retrieve inconsistent data due to ongoing updates \n",
      "taking place during the post-processing phase. Proposals on the best approach will be expected from \n",
      "the provider in charge of the technical specifications and development of the system5.  \n",
      "3.3.3 Perform Reference Data Content Validation \n",
      "Goal The goal of this use case is for individual records within a received file \n",
      "to be validated by ESMA.   \n",
      "Actors TV/SI (in the jurisdiction of a delegating NCA) - submits data \n",
      "NCA (not delegating data collection in its jurisdiction) - submits data \n",
      "The ESMA System – validates data\n",
      "\n",
      "--- Initial Result 5 ---\n",
      "\n",
      "as per contained in the Consistent Reference Data T able. In order to ensure security of the data \n",
      "contained in the Consistent Reference data table, the public user will access a copy of that table, the \n",
      "publication table, which is updated on daily basis during the publication process.\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T11:24:21.826946Z",
     "start_time": "2025-07-05T11:24:21.796668Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "#  STEP 5: RERANK RETRIEVED CHUNKS\n",
    "# ==============================================================================\n",
    "# The cross-encoder model will be downloaded on the first run.\n",
    "# It takes the query and a list of documents and returns them, scored and re-ordered.\n",
    "print(f\"\\n--- Initializing Reranker with '{RERANKER_MODEL_NAME}' ---\")\n",
    "model = HuggingFaceCrossEncoder(model_name=RERANKER_MODEL_NAME)\n",
    "reranker = CrossEncoderReranker(model=model, top_n=3)\n",
    "\n",
    "# 5d. Create the full retrieval pipeline with the reranker\n",
    "# The ContextualCompressionRetriever uses the base retriever to fetch documents\n",
    "# and then the reranker to re-order them based on relevance.\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=reranker, base_retriever=base_retriever\n",
    ")\n",
    "print(\"Reranking pipeline created.\")\n",
    "\n",
    "# 5e. Perform the final, reranked search\n",
    "print(\"\\n--- Performing search with reranking... ---\")\n",
    "reranked_chunks = compression_retriever.get_relevant_documents(USER_QUERY)\n",
    "print(\"\\n\\n=========================================================\")\n",
    "print(f\"--- Top 3 Reranked & Most Relevant Chunks ---\")\n",
    "print(\"=========================================================\")\n",
    "for i, chunk in enumerate(reranked_chunks, 1):\n",
    "    print(f\"\\n--- Final Result {i} ---\\n\")\n",
    "    print(chunk.page_content)"
   ],
   "id": "adf179ee1444f6d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Performing search with reranking... ---\n",
      "\n",
      "\n",
      "=========================================================\n",
      "--- Top 3 Reranked & Most Relevant Chunks ---\n",
      "=========================================================\n",
      "\n",
      "--- Final Result 1 ---\n",
      "\n",
      "ESMA REGULAR USE \n",
      " \n",
      " \n",
      "183 / 216 \n",
      " \n",
      "9 Annex 1c: Reference Data Content and \n",
      "Consistency Validation Rules  \n",
      "Control executed by the system \n",
      "Error \n",
      "code \n",
      "Error Message \n",
      "Concerned \n",
      "Fields \n",
      "The value of “Instrument Classification” shall \n",
      "be a valid ISO 10962 code and shall be \n",
      "covered by at least one of the CFI constructs \n",
      "in the CFI-based validation matrix. \n",
      "INS-101 \n",
      "The CFI code is not valid \n",
      "against the CFI based \n",
      "validation matrix. \n",
      "RTS field 3 against the list of \n",
      "valid CFI codes table and \n",
      "against the list of CFI Construct \n",
      "(Primary Key) in the CFI based \n",
      "validation table \n",
      "Check that Mandatory fields are reported \n",
      "according to “CFI-based validations table”. \n",
      "INS-102 The following mandatory \n",
      "fields are not reported: \n",
      "“List of RTS23 number Id \n",
      "of missing field(s)”.\n",
      "\n",
      "--- Final Result 2 ---\n",
      "\n",
      "address errors on previous submission.  \n",
      "Business \n",
      "Rules \n",
      "Table 33 - Reference Data Content and Consistency Validation Rules. \n",
      "Assumptions N/A \n",
      " \n",
      "3.3.4 Update the Received Reference Data Table \n",
      "Goal \n",
      "The goal of this use case is to update the Received Reference Data \n",
      "Table according to a submitted record which passed the content \n",
      "validation checks. \n",
      "Actors The ESMA System. \n",
      "Preconditions The ESMA System has performed the content validation on the submitted \n",
      "record.  \n",
      "Trigger \n",
      "The ESMA System has successfully validated the content of the submitted \n",
      "record. \n",
      "Postcondition The ESMA System has updated the Received Reference Data Table \n",
      "according to the submitted record. \n",
      "Normal Flow \n",
      "(Referenced \n",
      "records – \n",
      "DATINS file \n",
      "submission)\n",
      "\n",
      "--- Final Result 3 ---\n",
      "\n",
      "as per contained in the Consistent Reference Data T able. In order to ensure security of the data \n",
      "contained in the Consistent Reference data table, the public user will access a copy of that table, the \n",
      "publication table, which is updated on daily basis during the publication process.\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T11:43:38.557082Z",
     "start_time": "2025-07-05T11:43:38.554656Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Import dependencies\n",
    "import os\n",
    "from docling.backend.pypdfium2_backend import PyPdfiumDocumentBackend\n",
    "from docling.datamodel.accelerator_options import AcceleratorOptions, AcceleratorDevice\n",
    "from docling.datamodel.base_models import InputFormat\n",
    "from docling.datamodel.pipeline_options import PdfPipelineOptions\n",
    "from docling.document_converter import DocumentConverter, FormatOption\n",
    "from docling.pipeline.standard_pdf_pipeline import StandardPdfPipeline\n",
    "import time\n",
    "from pathlib import Path"
   ],
   "id": "95f963c966849d93",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T11:43:40.026585Z",
     "start_time": "2025-07-05T11:43:40.024652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# List of files\n",
    "current_dir = os.getcwd()\n",
    "data_dir = os.path.join(current_dir, \"resources\")\n",
    "files = os.listdir(data_dir)"
   ],
   "id": "6940900c27f875a3",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T11:44:09.437367Z",
     "start_time": "2025-07-05T11:44:09.435241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Pipeline configs\n",
    "accelerator_options = AcceleratorOptions(\n",
    "    num_threads=4, device=AcceleratorDevice.AUTO\n",
    ")\n",
    "pipeline_options = PdfPipelineOptions()\n",
    "pipeline_options.do_ocr = True\n",
    "pipeline_options.do_table_structure = True\n",
    "pipeline_options.table_structure_options.do_cell_matching = True\n",
    "pipeline_options.accelerator_options = accelerator_options"
   ],
   "id": "a2f8daa2f14660af",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T11:44:10.872450Z",
     "start_time": "2025-07-05T11:44:10.870233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Setup converter\n",
    "converted = DocumentConverter(\n",
    "    allowed_formats=[InputFormat.PDF],\n",
    "    format_options={\n",
    "        InputFormat.PDF: FormatOption(\n",
    "            pipeline_cls=StandardPdfPipeline,\n",
    "            pipeline_options=pipeline_options,\n",
    "            backend=PyPdfiumDocumentBackend\n",
    "        )\n",
    "    }\n",
    ")"
   ],
   "id": "a1aa7c3be5d09e1f",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T11:53:37.332782Z",
     "start_time": "2025-07-05T11:52:20.979533Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Begin parsing\n",
    "for file in files:\n",
    "    pdf_path = os.path.join(data_dir, file)\n",
    "    # Check if file exists\n",
    "    if not os.path.exists(pdf_path):\n",
    "        print(f\"Error: File '{pdf_path}' does not exist.\")\n",
    "        exit(1)\n",
    "    print(f\"Parsing file '{pdf_path}'...\")\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"Converting PDF to text...\")\n",
    "    conv_res = converted.convert(pdf_path)\n",
    "    print(\"Converting done.\")\n",
    "    output_dir = Path(\"parsed\")\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    doc_filename = conv_res.input.file.stem\n",
    "    # Save markdown\n",
    "    md_filename = output_dir / f\"{doc_filename}.md\"\n",
    "    conv_res.document.save_as_markdown(md_filename)\n",
    "    end_time = time.time() - start_time\n",
    "    print(f\"Parsing done. Time elapsed: {end_time:.2f} seconds.\")"
   ],
   "id": "ba0b36e3067cd613",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing file '/home/lewis/github/rag-strategies/resources/firds_reference_data_functional_specifications_v2.10.pdf'...\n",
      "Converting PDF to text...\n",
      "Converting done.\n",
      "Parsing done. Time elapsed: 76.35 seconds.\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T12:42:15.497888Z",
     "start_time": "2025-07-05T12:42:15.495475Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# --- DEBUGGING STEP ---\n",
    "# Print the current working directory to see where Python is looking.\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Get the API key from the environment variables\n",
    "# The string \"GENAI_API_KEY\" must match the variable name in your .env file\n",
    "api_key = os.getenv(\"GENAI_API_KEY\")\n",
    "\n",
    "# Check if the API key is loaded correctly\n",
    "if not api_key:\n",
    "    raise ValueError(\"No API key found. Please set the GENAI_API_KEY in your .env file.\")\n"
   ],
   "id": "3679ec8d6acfc2ea",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /home/lewis/github/rag-strategies\n"
     ]
    }
   ],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T12:46:35.949615Z",
     "start_time": "2025-07-05T12:46:35.947617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def read_pdf_as_bytes(file_path):\n",
    "    try:\n",
    "        with open(file_path, \"rb\") as file:\n",
    "            pdf_bytes = file.read()\n",
    "        return pdf_bytes\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{file_path}' not found.\")\n",
    "        return None"
   ],
   "id": "90ac0348cc5157c7",
   "outputs": [],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T12:46:38.825437Z",
     "start_time": "2025-07-05T12:46:38.822516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    try:\n",
    "        try:\n",
    "            from PyPDF2 import PdfReader\n",
    "        except ImportError:\n",
    "            from pypdf import PdfReader\n",
    "        full_text = \"\"\n",
    "        page_text = []\n",
    "\n",
    "        # Open and read PDF\n",
    "        with open(pdf_path, \"rb\") as file:\n",
    "            pdf_reader = PdfReader(file)\n",
    "\n",
    "            for page_num, page in enumerate(pdf_reader.pages):\n",
    "                text = page.extract_text()\n",
    "                page_text.append({\"page\": page_num + 1, \"text\": text})\n",
    "                full_text += f\"\\n\\n--- Page {page_num + 1} ---\\n\\n{text}\"\n",
    "        full_text_bytes = full_text.encode(\"utf-8\")\n",
    "        return read_pdf_as_bytes(pdf_path), page_text\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from PDF: {e}\")\n",
    "        return \"\", []"
   ],
   "id": "f9371f13ee341488",
   "outputs": [],
   "execution_count": 81
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T13:06:38.090258Z",
     "start_time": "2025-07-05T13:06:38.087520Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "\n",
    "def get_section_map_from_gemini(full_text):\n",
    "    print(\"Asking Gemini to identify the document structure...\")\n",
    "    prompt = \"\"\"\n",
    "    You are a technical document parser. Your task is to analyse the provided text from a PDF.\n",
    "    Identify all the file specification sections. A section typically starts with a pattern like \"d.dd XXXXX\", \"d.d XXXXXX\", \"d XXXXXXX\", or \"d Annex dd: XXXXXXXX\". These section headers are bolded.\n",
    "\n",
    "    Extract the following for each section found:\n",
    "    1. The full section title (e.g., '6.11 Rejection statistics table'.\n",
    "    2. The page number where the section title appears.\n",
    "\n",
    "    Return the result as a JSON array of objects. Each object should have two keys: 'title' and 'start_page'.\n",
    "    Ensure the page number is an integer.\n",
    "\n",
    "    Example of a single JSON object in the array:\n",
    "    {\n",
    "        \"section_title\": \"6.11 Rejection statistics table\",\n",
    "        \"start_page\": 10\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    client = genai.Client(api_key=api_key)\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-pro\",\n",
    "        config={\n",
    "            'temperature': 0.0,\n",
    "            'response_mime_type': 'application/json'\n",
    "        },\n",
    "        contents=[\n",
    "            types.Part.from_bytes(\n",
    "                data=full_text,\n",
    "                mime_type='application/pdf'\n",
    "            ),\n",
    "            prompt\n",
    "        ]\n",
    "    )\n",
    "    try:\n",
    "        section_map = json.loads(response.text)\n",
    "        print(f\"Gemini successfully identified {len(section_map)} sections.\")\n",
    "        return section_map\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Error: Gemini did not return a valid JSON response.\")\n",
    "        print(response.text)\n",
    "        return None"
   ],
   "id": "4cfe67ed98659e9d",
   "outputs": [],
   "execution_count": 90
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T13:06:40.988007Z",
     "start_time": "2025-07-05T13:06:40.985418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def create_logical_chunks(page_texts, section_map):\n",
    "    print(\"Creating logical chunks based on the section map...\")\n",
    "    text_by_page = {p[\"page\"]: p[\"text\"] for p in page_texts}\n",
    "\n",
    "    chunks = []\n",
    "    sorted_sections = sorted(section_map, key=lambda x: x[\"start_page\"])\n",
    "\n",
    "    for i, section in enumerate(sorted_sections):\n",
    "        start_page = section[\"start_page\"]\n",
    "        section_title = section[\"section_title\"]\n",
    "\n",
    "        end_page = None\n",
    "        if i + 1 < len(sorted_sections):\n",
    "            end_page = sorted_sections[i + 1][\"start_page\"] - 1\n",
    "\n",
    "        if end_page is None or end_page < start_page:\n",
    "            end_page = len(page_texts)\n",
    "\n",
    "        chunk_text = \"\"\n",
    "        # we use end_page + 2 to overlap with one additional page, to handle the case where a single page has 2 sections\n",
    "        for page_num in range(start_page, end_page + 2):\n",
    "            if page_num in text_by_page:\n",
    "                chunk_text += text_by_page[page_num] + \"\\n\\n\"\n",
    "\n",
    "        # Clean up the chunk: find the start of the current section text\n",
    "        title_pos = chunk_text.find(section_title)\n",
    "        if title_pos != -1:\n",
    "            chunk_text = chunk_text[title_pos:]\n",
    "\n",
    "        chunks.append({\n",
    "            \"section_title\": section_title,\n",
    "            \"text\": chunk_text.strip(),\n",
    "            \"start_page\": start_page,\n",
    "            \"end_page\": end_page\n",
    "        })\n",
    "\n",
    "        print(f\"Created {len(chunks)} logical chunks.\")\n",
    "        return chunks"
   ],
   "id": "4c601be02b4ec8a",
   "outputs": [],
   "execution_count": 91
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-05T13:09:48.473443Z",
     "start_time": "2025-07-05T13:07:57.808525Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ==============================================================================\n",
    "#  STEP 1: LOAD AND CHUNK THE DOCUMENT\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Step 1: Loading and Chunking PDF ---\")\n",
    "loader = PyPDFLoader(PDF_PATH)\n",
    "documents = loader.load()\n",
    "\n",
    "parsed_dir = \"parsed\"\n",
    "os.makedirs(parsed_dir, exist_ok=True)\n",
    "section_map_path = os.path.join(parsed_dir, \"section_map.json\")\n",
    "\n",
    "full_doc_text, pages = extract_text_from_pdf(PDF_PATH)\n",
    "if os.path.exists(section_map_path):\n",
    "    with open(section_map_path, \"r\") as f:\n",
    "        section_map = json.load(f)\n",
    "    print(\"Loaded existing section map.\")\n",
    "else:\n",
    "    section_map = get_section_map_from_gemini(full_doc_text)\n",
    "    if section_map:\n",
    "        with open(section_map_path, \"w\") as f:\n",
    "            json.dump(section_map, f, indent=2)\n",
    "        print(\"Saved section map to section_map.json.\")\n",
    "\n",
    "chunks = create_logical_chunks(pages, section_map)\n",
    "\n",
    "print(f\"PDF loaded and split into {len(chunks)} chunks.\")"
   ],
   "id": "505036da25727116",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Loading and Chunking PDF ---\n",
      "Asking Gemini to identify the document structure...\n",
      "Gemini successfully identified 171 sections.\n",
      "Saved section map to section_map.json.\n",
      "Creating logical chunks based on the section map...\n",
      "Created 1 logical chunks.\n",
      "PDF loaded and split into 1 chunks.\n"
     ]
    }
   ],
   "execution_count": 92
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# ==============================================================================\n",
    "#  STEP 2: EMBED THE CHUNKS\n",
    "# ==============================================================================\n",
    "print(f\"\\n--- Step 2: Embedding Chunks using '{EMBEDDING_MODEL_NAME}' ---\")\n",
    "# This will download the model from Hugging Face on its first run.\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL_NAME)\n",
    "print(\"Embedding model loaded.\")\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 3: STORE IN A VECTOR DATABASE\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Step 3: Storing chunks in FAISS in-memory vector database ---\")\n",
    "# The from_documents method handles embedding and storing in one step.\n",
    "vector_store = FAISS.from_documents(chunks, embeddings)\n",
    "print(\"Chunks embedded and stored in FAISS.\")\n",
    "\n",
    "# ==============================================================================\n",
    "#  STEP 4: RETRIEVE RELEVANT CHUNKS\n",
    "# ==============================================================================\n",
    "print(\"\\n--- Step 4: Retrieving Top 10 Chunks via Similarity Search ---\")\n",
    "print(f\"\\nUser Query: \\\"{USER_QUERY}\\\"\")\n",
    "\n",
    "# Retrieve the top 5 most similar chunks\n",
    "base_retriever = vector_store.as_retriever(search_kwargs={\"k\": 10})\n",
    "\n",
    "print(\"\\n--- Top 10 Retrieved Chunks ---\")\n",
    "initial_results = base_retriever.get_relevant_documents(USER_QUERY)\n",
    "print(f\"\\n--- Top 5 Initial Results (from vector search alone) ---\")\n",
    "for i, chunk in enumerate(initial_results[:5], 1):\n",
    "    print(f\"\\n--- Initial Result {i} ---\\n\")\n",
    "    print(chunk.page_content)"
   ],
   "id": "4603062b211032ca"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
